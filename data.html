<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>plaque_assay.data API documentation</title>
<meta name="description" content="Data I/O" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>plaque_assay.data</code></h1>
</header>
<section id="section-intro">
<p>Data I/O</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Data I/O
&#34;&#34;&#34;


from datetime import datetime, timezone
import logging
import os

import pandas as pd
import numpy as np

from . import utils
from . import consts
from . import db_models


def read_data_from_list(plate_list):
    &#34;&#34;&#34;Read in data from plate list and assign dilution values by well position.

    Notes
    ------
    This will mock the data so the 4 dilutions on a single 384-well
    plate are re-labelled to appear from 4 different 96 well plates.

    Parameters
    ----------
    plate_list : list

    Returns:
    ---------
    pandas.DataFrame
    &#34;&#34;&#34;
    dataframes = []
    barcodes = []
    for path in plate_list:
        df = pd.read_csv(
            # NOTE: might not always be Evaluation1
            os.path.join(path, &#34;Evaluation1/PlateResults.txt&#34;),
            skiprows=8,
            sep=&#34;\t&#34;,
        )
        plate_barcode = path.split(os.sep)[-1].split(&#34;__&#34;)[0]
        barcodes.append(plate_barcode)
        logging.info(&#34;plate barcode detected as %s&#34;, plate_barcode)
        well_labels = []
        for row, col in df[[&#34;Row&#34;, &#34;Column&#34;]].itertuples(index=False):
            well_labels.append(utils.row_col_to_well(row, col))
        df[&#34;Well&#34;] = well_labels
        df[&#34;Plate_barcode&#34;] = plate_barcode
        # Empty wells with no background produce NaNs rather than 0 in the
        # image analysis, which causes missing data for truely complete
        # inhbition. So we replace NaNs with 0 in the measurement columns we
        # use.
        fillna_cols = [&#34;Normalised Plaque area&#34;, &#34;Normalised Plaque intensity&#34;]
        for colname in fillna_cols:
            df[colname] = df[colname].fillna(0)
        dataframes.append(df)
    df_concat = pd.concat(dataframes)
    # NOTE: mock barcodes before changing wells
    df_concat[&#34;Plate_barcode&#34;] = utils.mock_384_barcode(
        existing_barcodes=df_concat[&#34;Plate_barcode&#34;], wells=df_concat[&#34;Well&#34;]
    )
    # mock wells
    df_concat[&#34;Well&#34;] = [utils.well_384_to_96(i) for i in df_concat[&#34;Well&#34;]]
    df_concat[&#34;PlateNum&#34;] = [int(i[1]) for i in df_concat[&#34;Plate_barcode&#34;]]
    df_concat[&#34;Dilution&#34;] = [consts.plate_mapping[i] for i in df_concat[&#34;PlateNum&#34;]]
    logging.debug(&#34;input data shape: %s&#34;, df_concat.shape)
    return df_concat


def get_plate_list(data_dir):
    &#34;&#34;&#34;Get paths to plate directories

    Parameters
    ----------
    data_dir : str
        path to the directory containing the plate sub-directories

    Returns
    -------
    list
        list of 2 paths to plate directories
    &#34;&#34;&#34;
    n_expected_plates = 2
    plate_list = [os.path.join(data_dir, i) for i in os.listdir(data_dir)]
    if len(plate_list) == n_expected_plates:
        logging.debug(&#34;plate list detected: %s&#34;, plate_list)
    else:
        logging.error(
            &#34;Did not detect %s plates, detected %s :&#34;,
            n_expected_plates,
            len(plate_list),
        )
    return plate_list


def read_data_from_directory(data_dir):
    &#34;&#34;&#34;Read actual barcoded plate directory

    This gets a plate list from `data_dir`, and then
    reads in data from the plate list into a single
    DataFrame.

    Parameters
    -----------
    data_dir : str

    Returns
    -------
    pandas.DataFrame
    &#34;&#34;&#34;
    plate_list = get_plate_list(data_dir)
    return read_data_from_list(plate_list)


def read_indexfiles_from_list(plate_list):
    &#34;&#34;&#34;Read indexfiles from a plate list

    Parameters
    ------------
    plate_list : list
        list of paths to plate directories

    Returns
    -------
    pandas.DataFrame
    &#34;&#34;&#34;
    dataframes = []
    for path in plate_list:
        df = pd.read_csv(os.path.join(path, &#34;indexfile.txt&#34;), sep=&#34;\t&#34;)
        plate_barcode = path.split(os.sep)[-1].split(&#34;__&#34;)[0]
        df[&#34;Plate_barcode&#34;] = plate_barcode
        dataframes.append(df)
    df_concat = pd.concat(dataframes)
    # remove annoying empty &#34;Unnamed: 16&#34; column
    to_rm = [col for col in df_concat.columns if col.startswith(&#34;Unnamed:&#34;)]
    df_concat.drop(to_rm, axis=1, inplace=True)
    if len(to_rm) &gt; 0:
        logging.info(&#34;removed columns: %s from concatenated indexfiles&#34;, to_rm)
    logging.debug(&#34;indexfile shape: %s&#34;, df_concat.shape)
    return df_concat


def read_indexfiles_from_directory(data_dir):
    &#34;&#34;&#34;Return dataframe of indexfiles from a directory containing plates.

    Parameters
    -----------
    data_dir : str

    Returns
    --------
    pandas.DataFrame
    &#34;&#34;&#34;
    plate_list = get_plate_list(data_dir)
    return read_indexfiles_from_list(plate_list)


class DatabaseUploader:
    &#34;&#34;&#34;
    Attributes
    ----------
    session : sql.orm.session.Session
        sqlalchemy session to LIMS serology database

    &#34;&#34;&#34;
    def __init__(self, session):
        self.session = session

    def commit(self):
        &#34;&#34;&#34;commit data to LIMS serology database&#34;&#34;&#34;
        self.session.commit()

    def already_uploaded(self, workflow_id, variant):
        &#34;&#34;&#34;
        Check if the results for a given workflow_id and variant
        have already been uploaded.

        As results are uploaded all-or-nothing, we can just check one of the
        results tables for the presence of the supplied workflow_id and
        variant, rather than each of the several results tables.

        Parameters
        -----------
        workflow_id : int
        variant : str

        Returns
        --------
        bool
        &#34;&#34;&#34;
        result = (
            self.session.query(db_models.NE_final_results)
            .filter(
                db_models.NE_final_results.workflow_id == workflow_id,
                db_models.NE_final_results.variant == variant,
            )
            .first()
        )
        return result is not None

    def is_final_upload(self, workflow_id):
        &#34;&#34;&#34;
        This determines if a given workflow_id is the last to be
        uploaded for that workflow_id. This is determined by the
        number of unique variants for a workflow_id currently in the
        LIMS database. If this is 1 short of the expected number, then
        we can determine the current upload is the final one.

        Parameters
        -----------
        workflow_id: int

        Returns
        ---------
        bool
            whether or not this is the final upload

        Raises
        ------
        RuntimeError
            when trying to upload more variants than specified in the
            workflow_tracking LIMS database table.
        &#34;&#34;&#34;
        # get expected number of variants from NE_workflow_tracking
        # fmt: off
        expected = (
            self.session
            .query(db_models.NE_workflow_tracking)
            .filter(db_models.NE_workflow_tracking.workflow_id == workflow_id)
            .first()
        )
        # get number of uploaded variants from NE_final_results
        current_variants = (
            self.session
            .query(db_models.NE_final_results.variant)
            .filter(db_models.NE_final_results.workflow_id == workflow_id)
        )
        # fmt: on
        current_n_variants = current_variants.distinct().count()
        # NOTE: the sqlalchemy queries are reading from NE_final_results data
        # that includes results from this session that have not yet been
        # committed, and as is_final_upload() is called *after*
        # upload_final_results(), we pretend the results are already in the
        # database.
        is_final = int(expected.no_of_variants) == int(current_n_variants)
        if int(current_n_variants) &gt; int(expected.no_of_variants):
            raise RuntimeError(
                f&#34;unexpected no. of variants {current_n_variants}, expecting max of {expected.no_of_variants}&#34;
            )
        logging.debug(f&#34;expected no. of variants: {expected.no_of_variants}&#34;)
        logging.debug(
            f&#34;current no. uploaded variants: {current_n_variants}: {current_variants}&#34;
        )
        if is_final:
            logging.info(
                f&#34;Final variant upload, marking workflow {workflow_id} as complete&#34;
            )
        else:
            logging.info(
                f&#34;Not final variant upload for workflow {workflow_id}, this is variant {current_n_variants}/{expected.no_of_variants}&#34;
            )
        return is_final

    def upload_plate_results(self, plate_results_dataset):
        &#34;&#34;&#34;Upload raw concatenated data into database.

        This uploads the &#34;raw&#34; dataset into the LIMS serology database.
        The data is largely what is exported from the Phenix with columns
        renamed, some columns moved, some metadata columns added.

        Parameters
        ----------
        plate_results_dataset: pandas.DataFrame

        Returns:
        ---------
        None
        &#34;&#34;&#34;
        # TODO: check csv matches master plate selected in NE_workflow_tracking
        plate_results_dataset = plate_results_dataset.copy()
        rename_dict = {
            &#34;Row&#34;: &#34;row&#34;,
            &#34;Column&#34;: &#34;column&#34;,
            &#34;Viral Plaques (global) - Area of Viral Plaques Area [µm²] - Mean per Well&#34;: &#34;VPG_area_mean&#34;,
            &#34;Viral Plaques (global) - Intensity Viral Plaques Alexa 488 (global) Mean - Mean per Well&#34;: &#34; VPG_intensity_mean_per_well&#34;,
            &#34;Viral Plaques (global) - Intensity Viral Plaques Alexa 488 (global) StdDev - Mean per Well&#34;: &#34;VPG_intensity_stddev_per_well&#34;,
            &#34;Viral Plaques (global) - Intensity Viral Plaques Alexa 488 (global) Median - Mean per Well&#34;: &#34;VPG_intensity_median_per_well&#34;,
            &#34;Viral Plaques (global) - Intensity Viral Plaques Alexa 488 (global) Sum - Mean per Well&#34;: &#34;VPG_intensity_sum_per_well&#34;,
            &#34;Cells - Intensity Image Region DAPI (global) Mean - Mean per Well&#34;: &#34;cells_intensity_mean_per_well&#34;,
            &#34;Cells - Intensity Image Region DAPI (global) StdDev - Mean per Well&#34;: &#34;cells_intensity_stddev_mean_per_well&#34;,
            &#34;Cells - Intensity Image Region DAPI (global) Median - Mean per Well&#34;: &#34;cells_intensity_median_mean_per_well&#34;,
            &#34;Cells - Intensity Image Region DAPI (global) Sum - Mean per Well&#34;: &#34;cells_intensity_sum_mean_per_well&#34;,
            &#34;Cells - Image Region Area [µm²] - Mean per Well&#34;: &#34;cells_image_region_area_mean_per_well&#34;,
            &#34;Normalised Plaque area&#34;: &#34;normalised_plaque_area&#34;,
            &#34;Normalised Plaque intensity&#34;: &#34;normalised_plaque_intensity&#34;,
            &#34;Number of Analyzed Fields&#34;: &#34;number_analyzed_fields&#34;,
            &#34;Dilution&#34;: &#34;dilution&#34;,
            &#34;Well&#34;: &#34;well&#34;,
            &#34;PlateNum&#34;: &#34;plate_num&#34;,
            &#34;Plate_barcode&#34;: &#34;plate_barcode&#34;,
            &#34;variant&#34;: &#34;variant&#34;,
            # &#34;Background Subtracted Plaque Area&#34;: &#34;background_subtracted_plaque_area&#34;,
        }
        plate_results_dataset.rename(columns=rename_dict, inplace=True)
        # filter to only desired columns
        plate_results_dataset = plate_results_dataset[list(rename_dict.values())]
        workflow_id = [int(i[3:]) for i in plate_results_dataset[&#34;plate_barcode&#34;]]
        plate_results_dataset[&#34;workflow_id&#34;] = workflow_id
        plate_results_dataset[&#34;well&#34;] = utils.unpad_well_col(
            plate_results_dataset[&#34;well&#34;]
        )
        # can&#39;t store NaN in mysql, to convert to None which are stored as null
        plate_results_dataset = plate_results_dataset.replace({np.nan: None})
        self.session.bulk_insert_mappings(
            db_models.NE_raw_results, plate_results_dataset.to_dict(orient=&#34;records&#34;)
        )

    def upload_indexfiles(self, indexfiles_dataset):
        &#34;&#34;&#34;Upload indexfiles from the Phenix into the database

        This uploads the IndexFile dataset into the LIMS serology database,
        which is kept because it contains the URLs to the images.

        Some columns are removed, most are renamed, and a variant metadata
        column is added.

        Parameters
        ----------
        indexfiles_dataset : pandas.DataFrame

        Returns
        -------
        None
        &#34;&#34;&#34;
        indexfiles_dataset = indexfiles_dataset.copy()
        rename_dict = {
            &#34;Row&#34;: &#34;row&#34;,
            &#34;Column&#34;: &#34;column&#34;,
            &#34;Field&#34;: &#34;field&#34;,
            &#34;Channel ID&#34;: &#34;channel_id&#34;,
            &#34;Channel Name&#34;: &#34;channel_name&#34;,
            &#34;Channel Type&#34;: &#34;channel_type&#34;,
            &#34;URL&#34;: &#34;url&#34;,
            &#34;ImageResolutionX [m]&#34;: &#34;image_resolutionx&#34;,
            &#34;ImageResolutionY [m]&#34;: &#34;image_resolutiony&#34;,
            &#34;ImageSizeX&#34;: &#34;image_sizex&#34;,
            &#34;ImageSizeY&#34;: &#34;image_sizey&#34;,
            &#34;PositionX [m]&#34;: &#34;positionx&#34;,
            &#34;PositionY [m]&#34;: &#34;positiony&#34;,
            &#34;Time Stamp&#34;: &#34;time_stamp&#34;,
            &#34;Plate_barcode&#34;: &#34;plate_barcode&#34;,
            &#34;variant&#34;: &#34;variant&#34;,  # not renamed, just to keep it
        }
        indexfiles_dataset.rename(columns=rename_dict, inplace=True)
        # filter to only desired columns
        indexfiles_dataset = indexfiles_dataset[list(rename_dict.values())]
        # get workflow ID
        workflow_id = [int(i[3:]) for i in indexfiles_dataset[&#34;plate_barcode&#34;]]
        indexfiles_dataset[&#34;workflow_id&#34;] = workflow_id
        # can&#39;t store NaN in mysql, to convert to None which are stored as null
        indexfiles_dataset = indexfiles_dataset.replace({np.nan: None})
        for i in range(0, len(indexfiles_dataset), 1000):
            df_slice = indexfiles_dataset.iloc[i : i + 1000]
            self.session.bulk_insert_mappings(
                db_models.NE_raw_index, df_slice.to_dict(orient=&#34;records&#34;)
            )

    def upload_normalised_results(self, norm_results):
        &#34;&#34;&#34;Upload normalised results into the database.

        Uploads the normalised data, consisting of 
        `background_subtracted_plaque_area`, `percentage_infected` and
        metadata, into the LIMS serology database.

        Parameters
        ----------
        norm_results: pandas.DataFrame

        Returns
        -------
        None
        &#34;&#34;&#34;
        norm_results = norm_results.copy()
        rename_dict = {
            &#34;Well&#34;: &#34;well&#34;,
            &#34;Row&#34;: &#34;row&#34;,
            &#34;Column&#34;: &#34;column&#34;,
            &#34;Dilution&#34;: &#34;dilution&#34;,
            &#34;Plate_barcode&#34;: &#34;plate_barcode&#34;,
            &#34;Background_subtracted_plaque_area&#34;: &#34;background_subtracted_plaque_area&#34;,
            &#34;Percentage_infected&#34;: &#34;percentage_infected&#34;,
            &#34;variant&#34;: &#34;variant&#34;,  # not renamed, just to keep
        }
        norm_results.rename(columns=rename_dict, inplace=True)
        norm_results = norm_results[list(rename_dict.values())]
        workflow_id = [int(i[3:]) for i in norm_results[&#34;plate_barcode&#34;]]
        assert len(set(workflow_id)) == 1
        norm_results[&#34;workflow_id&#34;] = workflow_id
        norm_results[&#34;well&#34;] = utils.unpad_well_col(norm_results[&#34;well&#34;])
        # can&#39;t store NaN in mysql, to convert to None which are stored as null
        norm_results = norm_results.replace({np.nan: None})
        self.session.bulk_insert_mappings(
            db_models.NE_normalized_results, norm_results.to_dict(orient=&#34;records&#34;)
        )

    def upload_final_results(self, results):
        &#34;&#34;&#34;Upload final results to database

        Final results are mainly IC50 and metadata.

        Parameters
        -----------
        results : pandas.DataFrame

        Returns
        -------
        None
        &#34;&#34;&#34;
        results = results.copy()
        # don&#39;t have master_plate details from accessible tables, set as None
        results[&#34;master_plate&#34;] = None
        # get workflow_id
        assert results[&#34;experiment&#34;].nunique() == 1
        assert results[&#34;variant&#34;].nunique() == 1
        results[&#34;workflow_id&#34;] = results[&#34;experiment&#34;].astype(int)
        # can&#39;t store NaN in mysql, to convert to None which are stored as null
        results = results.replace({np.nan: None})
        results[&#34;well&#34;] = utils.unpad_well_col(results[&#34;well&#34;])
        self.session.bulk_insert_mappings(
            db_models.NE_final_results, results.to_dict(orient=&#34;records&#34;)
        )

    def upload_failures(self, failures):
        &#34;&#34;&#34;Upload failure information to database

        Parameters
        ----------
        failures : pandas.DataFrame

        Returns
        -------
        None
        &#34;&#34;&#34;
        failures = failures.copy()
        if failures.shape[0] &gt; 0:
            assert failures[&#34;experiment&#34;].nunique() == 1
            failures[&#34;workflow_id&#34;] = failures[&#34;experiment&#34;].astype(int)
            self.session.bulk_insert_mappings(
                db_models.NE_failed_results, failures.to_dict(orient=&#34;records&#34;)
            )

    def upload_model_parameters(self, model_parameters):
        &#34;&#34;&#34;Upload model parameters to database

        Parameters
        -----------
        model_parameters: pandas.DataFrame

        Returns
        --------
        None
        &#34;&#34;&#34;
        model_parameters = model_parameters.copy()
        model_parameters.rename(columns={&#34;experiment&#34;: &#34;workflow_id&#34;}, inplace=True)
        # can&#39;t store NaNs
        model_parameters = model_parameters.replace({np.nan: None})
        model_parameters[&#34;well&#34;] = utils.unpad_well_col(model_parameters[&#34;well&#34;])
        self.session.bulk_insert_mappings(
            db_models.NE_model_parameters, model_parameters.to_dict(orient=&#34;records&#34;)
        )

    def update_workflow_tracking(self, workflow_id):
        &#34;&#34;&#34;Update workflow_tracking table to indicate all variants for
        a workflow have been uploaded.

        This doesn&#39;t check that a workflow is complete, that is handled
        by `DatabaseUploader.is_final_upload()`.

        Parameters
        -----------
        workflow_id: int

        Returns
        -------
        None
        &#34;&#34;&#34;
        # set status to &#34;complete&#34;
        # set final_results_upload to current datetime
        # set end_date to current datetime
        timestamp = datetime.now(timezone.utc)
        # fmt: off
        self.session\
            .query(db_models.NE_workflow_tracking)\
            .filter( db_models.NE_workflow_tracking.workflow_id == workflow_id)\
            .update(
                {
                    db_models.NE_workflow_tracking.status: &#34;complete&#34;,
                    db_models.NE_workflow_tracking.end_date: timestamp,
                    db_models.NE_workflow_tracking.final_results_upload: timestamp,
                }
            )
        # fmt: on</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="plaque_assay.data.get_plate_list"><code class="name flex">
<span>def <span class="ident">get_plate_list</span></span>(<span>data_dir)</span>
</code></dt>
<dd>
<div class="desc"><p>Get paths to plate directories</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data_dir</code></strong> :&ensp;<code>str</code></dt>
<dd>path to the directory containing the plate sub-directories</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code></dt>
<dd>list of 2 paths to plate directories</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_plate_list(data_dir):
    &#34;&#34;&#34;Get paths to plate directories

    Parameters
    ----------
    data_dir : str
        path to the directory containing the plate sub-directories

    Returns
    -------
    list
        list of 2 paths to plate directories
    &#34;&#34;&#34;
    n_expected_plates = 2
    plate_list = [os.path.join(data_dir, i) for i in os.listdir(data_dir)]
    if len(plate_list) == n_expected_plates:
        logging.debug(&#34;plate list detected: %s&#34;, plate_list)
    else:
        logging.error(
            &#34;Did not detect %s plates, detected %s :&#34;,
            n_expected_plates,
            len(plate_list),
        )
    return plate_list</code></pre>
</details>
</dd>
<dt id="plaque_assay.data.read_data_from_directory"><code class="name flex">
<span>def <span class="ident">read_data_from_directory</span></span>(<span>data_dir)</span>
</code></dt>
<dd>
<div class="desc"><p>Read actual barcoded plate directory</p>
<p>This gets a plate list from <code>data_dir</code>, and then
reads in data from the plate list into a single
DataFrame.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data_dir</code></strong> :&ensp;<code>str</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pandas.DataFrame</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_data_from_directory(data_dir):
    &#34;&#34;&#34;Read actual barcoded plate directory

    This gets a plate list from `data_dir`, and then
    reads in data from the plate list into a single
    DataFrame.

    Parameters
    -----------
    data_dir : str

    Returns
    -------
    pandas.DataFrame
    &#34;&#34;&#34;
    plate_list = get_plate_list(data_dir)
    return read_data_from_list(plate_list)</code></pre>
</details>
</dd>
<dt id="plaque_assay.data.read_data_from_list"><code class="name flex">
<span>def <span class="ident">read_data_from_list</span></span>(<span>plate_list)</span>
</code></dt>
<dd>
<div class="desc"><p>Read in data from plate list and assign dilution values by well position.</p>
<h2 id="notes">Notes</h2>
<p>This will mock the data so the 4 dilutions on a single 384-well
plate are re-labelled to appear from 4 different 96 well plates.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>plate_list</code></strong> :&ensp;<code>list</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns:</h2>
<p>pandas.DataFrame</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_data_from_list(plate_list):
    &#34;&#34;&#34;Read in data from plate list and assign dilution values by well position.

    Notes
    ------
    This will mock the data so the 4 dilutions on a single 384-well
    plate are re-labelled to appear from 4 different 96 well plates.

    Parameters
    ----------
    plate_list : list

    Returns:
    ---------
    pandas.DataFrame
    &#34;&#34;&#34;
    dataframes = []
    barcodes = []
    for path in plate_list:
        df = pd.read_csv(
            # NOTE: might not always be Evaluation1
            os.path.join(path, &#34;Evaluation1/PlateResults.txt&#34;),
            skiprows=8,
            sep=&#34;\t&#34;,
        )
        plate_barcode = path.split(os.sep)[-1].split(&#34;__&#34;)[0]
        barcodes.append(plate_barcode)
        logging.info(&#34;plate barcode detected as %s&#34;, plate_barcode)
        well_labels = []
        for row, col in df[[&#34;Row&#34;, &#34;Column&#34;]].itertuples(index=False):
            well_labels.append(utils.row_col_to_well(row, col))
        df[&#34;Well&#34;] = well_labels
        df[&#34;Plate_barcode&#34;] = plate_barcode
        # Empty wells with no background produce NaNs rather than 0 in the
        # image analysis, which causes missing data for truely complete
        # inhbition. So we replace NaNs with 0 in the measurement columns we
        # use.
        fillna_cols = [&#34;Normalised Plaque area&#34;, &#34;Normalised Plaque intensity&#34;]
        for colname in fillna_cols:
            df[colname] = df[colname].fillna(0)
        dataframes.append(df)
    df_concat = pd.concat(dataframes)
    # NOTE: mock barcodes before changing wells
    df_concat[&#34;Plate_barcode&#34;] = utils.mock_384_barcode(
        existing_barcodes=df_concat[&#34;Plate_barcode&#34;], wells=df_concat[&#34;Well&#34;]
    )
    # mock wells
    df_concat[&#34;Well&#34;] = [utils.well_384_to_96(i) for i in df_concat[&#34;Well&#34;]]
    df_concat[&#34;PlateNum&#34;] = [int(i[1]) for i in df_concat[&#34;Plate_barcode&#34;]]
    df_concat[&#34;Dilution&#34;] = [consts.plate_mapping[i] for i in df_concat[&#34;PlateNum&#34;]]
    logging.debug(&#34;input data shape: %s&#34;, df_concat.shape)
    return df_concat</code></pre>
</details>
</dd>
<dt id="plaque_assay.data.read_indexfiles_from_directory"><code class="name flex">
<span>def <span class="ident">read_indexfiles_from_directory</span></span>(<span>data_dir)</span>
</code></dt>
<dd>
<div class="desc"><p>Return dataframe of indexfiles from a directory containing plates.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data_dir</code></strong> :&ensp;<code>str</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pandas.DataFrame</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_indexfiles_from_directory(data_dir):
    &#34;&#34;&#34;Return dataframe of indexfiles from a directory containing plates.

    Parameters
    -----------
    data_dir : str

    Returns
    --------
    pandas.DataFrame
    &#34;&#34;&#34;
    plate_list = get_plate_list(data_dir)
    return read_indexfiles_from_list(plate_list)</code></pre>
</details>
</dd>
<dt id="plaque_assay.data.read_indexfiles_from_list"><code class="name flex">
<span>def <span class="ident">read_indexfiles_from_list</span></span>(<span>plate_list)</span>
</code></dt>
<dd>
<div class="desc"><p>Read indexfiles from a plate list</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>plate_list</code></strong> :&ensp;<code>list</code></dt>
<dd>list of paths to plate directories</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pandas.DataFrame</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_indexfiles_from_list(plate_list):
    &#34;&#34;&#34;Read indexfiles from a plate list

    Parameters
    ------------
    plate_list : list
        list of paths to plate directories

    Returns
    -------
    pandas.DataFrame
    &#34;&#34;&#34;
    dataframes = []
    for path in plate_list:
        df = pd.read_csv(os.path.join(path, &#34;indexfile.txt&#34;), sep=&#34;\t&#34;)
        plate_barcode = path.split(os.sep)[-1].split(&#34;__&#34;)[0]
        df[&#34;Plate_barcode&#34;] = plate_barcode
        dataframes.append(df)
    df_concat = pd.concat(dataframes)
    # remove annoying empty &#34;Unnamed: 16&#34; column
    to_rm = [col for col in df_concat.columns if col.startswith(&#34;Unnamed:&#34;)]
    df_concat.drop(to_rm, axis=1, inplace=True)
    if len(to_rm) &gt; 0:
        logging.info(&#34;removed columns: %s from concatenated indexfiles&#34;, to_rm)
    logging.debug(&#34;indexfile shape: %s&#34;, df_concat.shape)
    return df_concat</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="plaque_assay.data.DatabaseUploader"><code class="flex name class">
<span>class <span class="ident">DatabaseUploader</span></span>
<span>(</span><span>session)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>session</code></strong> :&ensp;<code>sql.orm.session.Session</code></dt>
<dd>sqlalchemy session to LIMS serology database</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DatabaseUploader:
    &#34;&#34;&#34;
    Attributes
    ----------
    session : sql.orm.session.Session
        sqlalchemy session to LIMS serology database

    &#34;&#34;&#34;
    def __init__(self, session):
        self.session = session

    def commit(self):
        &#34;&#34;&#34;commit data to LIMS serology database&#34;&#34;&#34;
        self.session.commit()

    def already_uploaded(self, workflow_id, variant):
        &#34;&#34;&#34;
        Check if the results for a given workflow_id and variant
        have already been uploaded.

        As results are uploaded all-or-nothing, we can just check one of the
        results tables for the presence of the supplied workflow_id and
        variant, rather than each of the several results tables.

        Parameters
        -----------
        workflow_id : int
        variant : str

        Returns
        --------
        bool
        &#34;&#34;&#34;
        result = (
            self.session.query(db_models.NE_final_results)
            .filter(
                db_models.NE_final_results.workflow_id == workflow_id,
                db_models.NE_final_results.variant == variant,
            )
            .first()
        )
        return result is not None

    def is_final_upload(self, workflow_id):
        &#34;&#34;&#34;
        This determines if a given workflow_id is the last to be
        uploaded for that workflow_id. This is determined by the
        number of unique variants for a workflow_id currently in the
        LIMS database. If this is 1 short of the expected number, then
        we can determine the current upload is the final one.

        Parameters
        -----------
        workflow_id: int

        Returns
        ---------
        bool
            whether or not this is the final upload

        Raises
        ------
        RuntimeError
            when trying to upload more variants than specified in the
            workflow_tracking LIMS database table.
        &#34;&#34;&#34;
        # get expected number of variants from NE_workflow_tracking
        # fmt: off
        expected = (
            self.session
            .query(db_models.NE_workflow_tracking)
            .filter(db_models.NE_workflow_tracking.workflow_id == workflow_id)
            .first()
        )
        # get number of uploaded variants from NE_final_results
        current_variants = (
            self.session
            .query(db_models.NE_final_results.variant)
            .filter(db_models.NE_final_results.workflow_id == workflow_id)
        )
        # fmt: on
        current_n_variants = current_variants.distinct().count()
        # NOTE: the sqlalchemy queries are reading from NE_final_results data
        # that includes results from this session that have not yet been
        # committed, and as is_final_upload() is called *after*
        # upload_final_results(), we pretend the results are already in the
        # database.
        is_final = int(expected.no_of_variants) == int(current_n_variants)
        if int(current_n_variants) &gt; int(expected.no_of_variants):
            raise RuntimeError(
                f&#34;unexpected no. of variants {current_n_variants}, expecting max of {expected.no_of_variants}&#34;
            )
        logging.debug(f&#34;expected no. of variants: {expected.no_of_variants}&#34;)
        logging.debug(
            f&#34;current no. uploaded variants: {current_n_variants}: {current_variants}&#34;
        )
        if is_final:
            logging.info(
                f&#34;Final variant upload, marking workflow {workflow_id} as complete&#34;
            )
        else:
            logging.info(
                f&#34;Not final variant upload for workflow {workflow_id}, this is variant {current_n_variants}/{expected.no_of_variants}&#34;
            )
        return is_final

    def upload_plate_results(self, plate_results_dataset):
        &#34;&#34;&#34;Upload raw concatenated data into database.

        This uploads the &#34;raw&#34; dataset into the LIMS serology database.
        The data is largely what is exported from the Phenix with columns
        renamed, some columns moved, some metadata columns added.

        Parameters
        ----------
        plate_results_dataset: pandas.DataFrame

        Returns:
        ---------
        None
        &#34;&#34;&#34;
        # TODO: check csv matches master plate selected in NE_workflow_tracking
        plate_results_dataset = plate_results_dataset.copy()
        rename_dict = {
            &#34;Row&#34;: &#34;row&#34;,
            &#34;Column&#34;: &#34;column&#34;,
            &#34;Viral Plaques (global) - Area of Viral Plaques Area [µm²] - Mean per Well&#34;: &#34;VPG_area_mean&#34;,
            &#34;Viral Plaques (global) - Intensity Viral Plaques Alexa 488 (global) Mean - Mean per Well&#34;: &#34; VPG_intensity_mean_per_well&#34;,
            &#34;Viral Plaques (global) - Intensity Viral Plaques Alexa 488 (global) StdDev - Mean per Well&#34;: &#34;VPG_intensity_stddev_per_well&#34;,
            &#34;Viral Plaques (global) - Intensity Viral Plaques Alexa 488 (global) Median - Mean per Well&#34;: &#34;VPG_intensity_median_per_well&#34;,
            &#34;Viral Plaques (global) - Intensity Viral Plaques Alexa 488 (global) Sum - Mean per Well&#34;: &#34;VPG_intensity_sum_per_well&#34;,
            &#34;Cells - Intensity Image Region DAPI (global) Mean - Mean per Well&#34;: &#34;cells_intensity_mean_per_well&#34;,
            &#34;Cells - Intensity Image Region DAPI (global) StdDev - Mean per Well&#34;: &#34;cells_intensity_stddev_mean_per_well&#34;,
            &#34;Cells - Intensity Image Region DAPI (global) Median - Mean per Well&#34;: &#34;cells_intensity_median_mean_per_well&#34;,
            &#34;Cells - Intensity Image Region DAPI (global) Sum - Mean per Well&#34;: &#34;cells_intensity_sum_mean_per_well&#34;,
            &#34;Cells - Image Region Area [µm²] - Mean per Well&#34;: &#34;cells_image_region_area_mean_per_well&#34;,
            &#34;Normalised Plaque area&#34;: &#34;normalised_plaque_area&#34;,
            &#34;Normalised Plaque intensity&#34;: &#34;normalised_plaque_intensity&#34;,
            &#34;Number of Analyzed Fields&#34;: &#34;number_analyzed_fields&#34;,
            &#34;Dilution&#34;: &#34;dilution&#34;,
            &#34;Well&#34;: &#34;well&#34;,
            &#34;PlateNum&#34;: &#34;plate_num&#34;,
            &#34;Plate_barcode&#34;: &#34;plate_barcode&#34;,
            &#34;variant&#34;: &#34;variant&#34;,
            # &#34;Background Subtracted Plaque Area&#34;: &#34;background_subtracted_plaque_area&#34;,
        }
        plate_results_dataset.rename(columns=rename_dict, inplace=True)
        # filter to only desired columns
        plate_results_dataset = plate_results_dataset[list(rename_dict.values())]
        workflow_id = [int(i[3:]) for i in plate_results_dataset[&#34;plate_barcode&#34;]]
        plate_results_dataset[&#34;workflow_id&#34;] = workflow_id
        plate_results_dataset[&#34;well&#34;] = utils.unpad_well_col(
            plate_results_dataset[&#34;well&#34;]
        )
        # can&#39;t store NaN in mysql, to convert to None which are stored as null
        plate_results_dataset = plate_results_dataset.replace({np.nan: None})
        self.session.bulk_insert_mappings(
            db_models.NE_raw_results, plate_results_dataset.to_dict(orient=&#34;records&#34;)
        )

    def upload_indexfiles(self, indexfiles_dataset):
        &#34;&#34;&#34;Upload indexfiles from the Phenix into the database

        This uploads the IndexFile dataset into the LIMS serology database,
        which is kept because it contains the URLs to the images.

        Some columns are removed, most are renamed, and a variant metadata
        column is added.

        Parameters
        ----------
        indexfiles_dataset : pandas.DataFrame

        Returns
        -------
        None
        &#34;&#34;&#34;
        indexfiles_dataset = indexfiles_dataset.copy()
        rename_dict = {
            &#34;Row&#34;: &#34;row&#34;,
            &#34;Column&#34;: &#34;column&#34;,
            &#34;Field&#34;: &#34;field&#34;,
            &#34;Channel ID&#34;: &#34;channel_id&#34;,
            &#34;Channel Name&#34;: &#34;channel_name&#34;,
            &#34;Channel Type&#34;: &#34;channel_type&#34;,
            &#34;URL&#34;: &#34;url&#34;,
            &#34;ImageResolutionX [m]&#34;: &#34;image_resolutionx&#34;,
            &#34;ImageResolutionY [m]&#34;: &#34;image_resolutiony&#34;,
            &#34;ImageSizeX&#34;: &#34;image_sizex&#34;,
            &#34;ImageSizeY&#34;: &#34;image_sizey&#34;,
            &#34;PositionX [m]&#34;: &#34;positionx&#34;,
            &#34;PositionY [m]&#34;: &#34;positiony&#34;,
            &#34;Time Stamp&#34;: &#34;time_stamp&#34;,
            &#34;Plate_barcode&#34;: &#34;plate_barcode&#34;,
            &#34;variant&#34;: &#34;variant&#34;,  # not renamed, just to keep it
        }
        indexfiles_dataset.rename(columns=rename_dict, inplace=True)
        # filter to only desired columns
        indexfiles_dataset = indexfiles_dataset[list(rename_dict.values())]
        # get workflow ID
        workflow_id = [int(i[3:]) for i in indexfiles_dataset[&#34;plate_barcode&#34;]]
        indexfiles_dataset[&#34;workflow_id&#34;] = workflow_id
        # can&#39;t store NaN in mysql, to convert to None which are stored as null
        indexfiles_dataset = indexfiles_dataset.replace({np.nan: None})
        for i in range(0, len(indexfiles_dataset), 1000):
            df_slice = indexfiles_dataset.iloc[i : i + 1000]
            self.session.bulk_insert_mappings(
                db_models.NE_raw_index, df_slice.to_dict(orient=&#34;records&#34;)
            )

    def upload_normalised_results(self, norm_results):
        &#34;&#34;&#34;Upload normalised results into the database.

        Uploads the normalised data, consisting of 
        `background_subtracted_plaque_area`, `percentage_infected` and
        metadata, into the LIMS serology database.

        Parameters
        ----------
        norm_results: pandas.DataFrame

        Returns
        -------
        None
        &#34;&#34;&#34;
        norm_results = norm_results.copy()
        rename_dict = {
            &#34;Well&#34;: &#34;well&#34;,
            &#34;Row&#34;: &#34;row&#34;,
            &#34;Column&#34;: &#34;column&#34;,
            &#34;Dilution&#34;: &#34;dilution&#34;,
            &#34;Plate_barcode&#34;: &#34;plate_barcode&#34;,
            &#34;Background_subtracted_plaque_area&#34;: &#34;background_subtracted_plaque_area&#34;,
            &#34;Percentage_infected&#34;: &#34;percentage_infected&#34;,
            &#34;variant&#34;: &#34;variant&#34;,  # not renamed, just to keep
        }
        norm_results.rename(columns=rename_dict, inplace=True)
        norm_results = norm_results[list(rename_dict.values())]
        workflow_id = [int(i[3:]) for i in norm_results[&#34;plate_barcode&#34;]]
        assert len(set(workflow_id)) == 1
        norm_results[&#34;workflow_id&#34;] = workflow_id
        norm_results[&#34;well&#34;] = utils.unpad_well_col(norm_results[&#34;well&#34;])
        # can&#39;t store NaN in mysql, to convert to None which are stored as null
        norm_results = norm_results.replace({np.nan: None})
        self.session.bulk_insert_mappings(
            db_models.NE_normalized_results, norm_results.to_dict(orient=&#34;records&#34;)
        )

    def upload_final_results(self, results):
        &#34;&#34;&#34;Upload final results to database

        Final results are mainly IC50 and metadata.

        Parameters
        -----------
        results : pandas.DataFrame

        Returns
        -------
        None
        &#34;&#34;&#34;
        results = results.copy()
        # don&#39;t have master_plate details from accessible tables, set as None
        results[&#34;master_plate&#34;] = None
        # get workflow_id
        assert results[&#34;experiment&#34;].nunique() == 1
        assert results[&#34;variant&#34;].nunique() == 1
        results[&#34;workflow_id&#34;] = results[&#34;experiment&#34;].astype(int)
        # can&#39;t store NaN in mysql, to convert to None which are stored as null
        results = results.replace({np.nan: None})
        results[&#34;well&#34;] = utils.unpad_well_col(results[&#34;well&#34;])
        self.session.bulk_insert_mappings(
            db_models.NE_final_results, results.to_dict(orient=&#34;records&#34;)
        )

    def upload_failures(self, failures):
        &#34;&#34;&#34;Upload failure information to database

        Parameters
        ----------
        failures : pandas.DataFrame

        Returns
        -------
        None
        &#34;&#34;&#34;
        failures = failures.copy()
        if failures.shape[0] &gt; 0:
            assert failures[&#34;experiment&#34;].nunique() == 1
            failures[&#34;workflow_id&#34;] = failures[&#34;experiment&#34;].astype(int)
            self.session.bulk_insert_mappings(
                db_models.NE_failed_results, failures.to_dict(orient=&#34;records&#34;)
            )

    def upload_model_parameters(self, model_parameters):
        &#34;&#34;&#34;Upload model parameters to database

        Parameters
        -----------
        model_parameters: pandas.DataFrame

        Returns
        --------
        None
        &#34;&#34;&#34;
        model_parameters = model_parameters.copy()
        model_parameters.rename(columns={&#34;experiment&#34;: &#34;workflow_id&#34;}, inplace=True)
        # can&#39;t store NaNs
        model_parameters = model_parameters.replace({np.nan: None})
        model_parameters[&#34;well&#34;] = utils.unpad_well_col(model_parameters[&#34;well&#34;])
        self.session.bulk_insert_mappings(
            db_models.NE_model_parameters, model_parameters.to_dict(orient=&#34;records&#34;)
        )

    def update_workflow_tracking(self, workflow_id):
        &#34;&#34;&#34;Update workflow_tracking table to indicate all variants for
        a workflow have been uploaded.

        This doesn&#39;t check that a workflow is complete, that is handled
        by `DatabaseUploader.is_final_upload()`.

        Parameters
        -----------
        workflow_id: int

        Returns
        -------
        None
        &#34;&#34;&#34;
        # set status to &#34;complete&#34;
        # set final_results_upload to current datetime
        # set end_date to current datetime
        timestamp = datetime.now(timezone.utc)
        # fmt: off
        self.session\
            .query(db_models.NE_workflow_tracking)\
            .filter( db_models.NE_workflow_tracking.workflow_id == workflow_id)\
            .update(
                {
                    db_models.NE_workflow_tracking.status: &#34;complete&#34;,
                    db_models.NE_workflow_tracking.end_date: timestamp,
                    db_models.NE_workflow_tracking.final_results_upload: timestamp,
                }
            )
        # fmt: on</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="plaque_assay.data.DatabaseUploader.already_uploaded"><code class="name flex">
<span>def <span class="ident">already_uploaded</span></span>(<span>self, workflow_id, variant)</span>
</code></dt>
<dd>
<div class="desc"><p>Check if the results for a given workflow_id and variant
have already been uploaded.</p>
<p>As results are uploaded all-or-nothing, we can just check one of the
results tables for the presence of the supplied workflow_id and
variant, rather than each of the several results tables.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>workflow_id</code></strong> :&ensp;<code>int</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>variant</code></strong> :&ensp;<code>str</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>bool</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def already_uploaded(self, workflow_id, variant):
    &#34;&#34;&#34;
    Check if the results for a given workflow_id and variant
    have already been uploaded.

    As results are uploaded all-or-nothing, we can just check one of the
    results tables for the presence of the supplied workflow_id and
    variant, rather than each of the several results tables.

    Parameters
    -----------
    workflow_id : int
    variant : str

    Returns
    --------
    bool
    &#34;&#34;&#34;
    result = (
        self.session.query(db_models.NE_final_results)
        .filter(
            db_models.NE_final_results.workflow_id == workflow_id,
            db_models.NE_final_results.variant == variant,
        )
        .first()
    )
    return result is not None</code></pre>
</details>
</dd>
<dt id="plaque_assay.data.DatabaseUploader.commit"><code class="name flex">
<span>def <span class="ident">commit</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>commit data to LIMS serology database</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def commit(self):
    &#34;&#34;&#34;commit data to LIMS serology database&#34;&#34;&#34;
    self.session.commit()</code></pre>
</details>
</dd>
<dt id="plaque_assay.data.DatabaseUploader.is_final_upload"><code class="name flex">
<span>def <span class="ident">is_final_upload</span></span>(<span>self, workflow_id)</span>
</code></dt>
<dd>
<div class="desc"><p>This determines if a given workflow_id is the last to be
uploaded for that workflow_id. This is determined by the
number of unique variants for a workflow_id currently in the
LIMS database. If this is 1 short of the expected number, then
we can determine the current upload is the final one.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>workflow_id</code></strong> :&ensp;<code>int</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>bool</code></dt>
<dd>whether or not this is the final upload</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>RuntimeError</code></dt>
<dd>when trying to upload more variants than specified in the
workflow_tracking LIMS database table.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_final_upload(self, workflow_id):
    &#34;&#34;&#34;
    This determines if a given workflow_id is the last to be
    uploaded for that workflow_id. This is determined by the
    number of unique variants for a workflow_id currently in the
    LIMS database. If this is 1 short of the expected number, then
    we can determine the current upload is the final one.

    Parameters
    -----------
    workflow_id: int

    Returns
    ---------
    bool
        whether or not this is the final upload

    Raises
    ------
    RuntimeError
        when trying to upload more variants than specified in the
        workflow_tracking LIMS database table.
    &#34;&#34;&#34;
    # get expected number of variants from NE_workflow_tracking
    # fmt: off
    expected = (
        self.session
        .query(db_models.NE_workflow_tracking)
        .filter(db_models.NE_workflow_tracking.workflow_id == workflow_id)
        .first()
    )
    # get number of uploaded variants from NE_final_results
    current_variants = (
        self.session
        .query(db_models.NE_final_results.variant)
        .filter(db_models.NE_final_results.workflow_id == workflow_id)
    )
    # fmt: on
    current_n_variants = current_variants.distinct().count()
    # NOTE: the sqlalchemy queries are reading from NE_final_results data
    # that includes results from this session that have not yet been
    # committed, and as is_final_upload() is called *after*
    # upload_final_results(), we pretend the results are already in the
    # database.
    is_final = int(expected.no_of_variants) == int(current_n_variants)
    if int(current_n_variants) &gt; int(expected.no_of_variants):
        raise RuntimeError(
            f&#34;unexpected no. of variants {current_n_variants}, expecting max of {expected.no_of_variants}&#34;
        )
    logging.debug(f&#34;expected no. of variants: {expected.no_of_variants}&#34;)
    logging.debug(
        f&#34;current no. uploaded variants: {current_n_variants}: {current_variants}&#34;
    )
    if is_final:
        logging.info(
            f&#34;Final variant upload, marking workflow {workflow_id} as complete&#34;
        )
    else:
        logging.info(
            f&#34;Not final variant upload for workflow {workflow_id}, this is variant {current_n_variants}/{expected.no_of_variants}&#34;
        )
    return is_final</code></pre>
</details>
</dd>
<dt id="plaque_assay.data.DatabaseUploader.update_workflow_tracking"><code class="name flex">
<span>def <span class="ident">update_workflow_tracking</span></span>(<span>self, workflow_id)</span>
</code></dt>
<dd>
<div class="desc"><p>Update workflow_tracking table to indicate all variants for
a workflow have been uploaded.</p>
<p>This doesn't check that a workflow is complete, that is handled
by <code><a title="plaque_assay.data.DatabaseUploader.is_final_upload" href="#plaque_assay.data.DatabaseUploader.is_final_upload">DatabaseUploader.is_final_upload()</a></code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>workflow_id</code></strong> :&ensp;<code>int</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_workflow_tracking(self, workflow_id):
    &#34;&#34;&#34;Update workflow_tracking table to indicate all variants for
    a workflow have been uploaded.

    This doesn&#39;t check that a workflow is complete, that is handled
    by `DatabaseUploader.is_final_upload()`.

    Parameters
    -----------
    workflow_id: int

    Returns
    -------
    None
    &#34;&#34;&#34;
    # set status to &#34;complete&#34;
    # set final_results_upload to current datetime
    # set end_date to current datetime
    timestamp = datetime.now(timezone.utc)
    # fmt: off
    self.session\
        .query(db_models.NE_workflow_tracking)\
        .filter( db_models.NE_workflow_tracking.workflow_id == workflow_id)\
        .update(
            {
                db_models.NE_workflow_tracking.status: &#34;complete&#34;,
                db_models.NE_workflow_tracking.end_date: timestamp,
                db_models.NE_workflow_tracking.final_results_upload: timestamp,
            }
        )
    # fmt: on</code></pre>
</details>
</dd>
<dt id="plaque_assay.data.DatabaseUploader.upload_failures"><code class="name flex">
<span>def <span class="ident">upload_failures</span></span>(<span>self, failures)</span>
</code></dt>
<dd>
<div class="desc"><p>Upload failure information to database</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>failures</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def upload_failures(self, failures):
    &#34;&#34;&#34;Upload failure information to database

    Parameters
    ----------
    failures : pandas.DataFrame

    Returns
    -------
    None
    &#34;&#34;&#34;
    failures = failures.copy()
    if failures.shape[0] &gt; 0:
        assert failures[&#34;experiment&#34;].nunique() == 1
        failures[&#34;workflow_id&#34;] = failures[&#34;experiment&#34;].astype(int)
        self.session.bulk_insert_mappings(
            db_models.NE_failed_results, failures.to_dict(orient=&#34;records&#34;)
        )</code></pre>
</details>
</dd>
<dt id="plaque_assay.data.DatabaseUploader.upload_final_results"><code class="name flex">
<span>def <span class="ident">upload_final_results</span></span>(<span>self, results)</span>
</code></dt>
<dd>
<div class="desc"><p>Upload final results to database</p>
<p>Final results are mainly IC50 and metadata.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>results</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def upload_final_results(self, results):
    &#34;&#34;&#34;Upload final results to database

    Final results are mainly IC50 and metadata.

    Parameters
    -----------
    results : pandas.DataFrame

    Returns
    -------
    None
    &#34;&#34;&#34;
    results = results.copy()
    # don&#39;t have master_plate details from accessible tables, set as None
    results[&#34;master_plate&#34;] = None
    # get workflow_id
    assert results[&#34;experiment&#34;].nunique() == 1
    assert results[&#34;variant&#34;].nunique() == 1
    results[&#34;workflow_id&#34;] = results[&#34;experiment&#34;].astype(int)
    # can&#39;t store NaN in mysql, to convert to None which are stored as null
    results = results.replace({np.nan: None})
    results[&#34;well&#34;] = utils.unpad_well_col(results[&#34;well&#34;])
    self.session.bulk_insert_mappings(
        db_models.NE_final_results, results.to_dict(orient=&#34;records&#34;)
    )</code></pre>
</details>
</dd>
<dt id="plaque_assay.data.DatabaseUploader.upload_indexfiles"><code class="name flex">
<span>def <span class="ident">upload_indexfiles</span></span>(<span>self, indexfiles_dataset)</span>
</code></dt>
<dd>
<div class="desc"><p>Upload indexfiles from the Phenix into the database</p>
<p>This uploads the IndexFile dataset into the LIMS serology database,
which is kept because it contains the URLs to the images.</p>
<p>Some columns are removed, most are renamed, and a variant metadata
column is added.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>indexfiles_dataset</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def upload_indexfiles(self, indexfiles_dataset):
    &#34;&#34;&#34;Upload indexfiles from the Phenix into the database

    This uploads the IndexFile dataset into the LIMS serology database,
    which is kept because it contains the URLs to the images.

    Some columns are removed, most are renamed, and a variant metadata
    column is added.

    Parameters
    ----------
    indexfiles_dataset : pandas.DataFrame

    Returns
    -------
    None
    &#34;&#34;&#34;
    indexfiles_dataset = indexfiles_dataset.copy()
    rename_dict = {
        &#34;Row&#34;: &#34;row&#34;,
        &#34;Column&#34;: &#34;column&#34;,
        &#34;Field&#34;: &#34;field&#34;,
        &#34;Channel ID&#34;: &#34;channel_id&#34;,
        &#34;Channel Name&#34;: &#34;channel_name&#34;,
        &#34;Channel Type&#34;: &#34;channel_type&#34;,
        &#34;URL&#34;: &#34;url&#34;,
        &#34;ImageResolutionX [m]&#34;: &#34;image_resolutionx&#34;,
        &#34;ImageResolutionY [m]&#34;: &#34;image_resolutiony&#34;,
        &#34;ImageSizeX&#34;: &#34;image_sizex&#34;,
        &#34;ImageSizeY&#34;: &#34;image_sizey&#34;,
        &#34;PositionX [m]&#34;: &#34;positionx&#34;,
        &#34;PositionY [m]&#34;: &#34;positiony&#34;,
        &#34;Time Stamp&#34;: &#34;time_stamp&#34;,
        &#34;Plate_barcode&#34;: &#34;plate_barcode&#34;,
        &#34;variant&#34;: &#34;variant&#34;,  # not renamed, just to keep it
    }
    indexfiles_dataset.rename(columns=rename_dict, inplace=True)
    # filter to only desired columns
    indexfiles_dataset = indexfiles_dataset[list(rename_dict.values())]
    # get workflow ID
    workflow_id = [int(i[3:]) for i in indexfiles_dataset[&#34;plate_barcode&#34;]]
    indexfiles_dataset[&#34;workflow_id&#34;] = workflow_id
    # can&#39;t store NaN in mysql, to convert to None which are stored as null
    indexfiles_dataset = indexfiles_dataset.replace({np.nan: None})
    for i in range(0, len(indexfiles_dataset), 1000):
        df_slice = indexfiles_dataset.iloc[i : i + 1000]
        self.session.bulk_insert_mappings(
            db_models.NE_raw_index, df_slice.to_dict(orient=&#34;records&#34;)
        )</code></pre>
</details>
</dd>
<dt id="plaque_assay.data.DatabaseUploader.upload_model_parameters"><code class="name flex">
<span>def <span class="ident">upload_model_parameters</span></span>(<span>self, model_parameters)</span>
</code></dt>
<dd>
<div class="desc"><p>Upload model parameters to database</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model_parameters</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def upload_model_parameters(self, model_parameters):
    &#34;&#34;&#34;Upload model parameters to database

    Parameters
    -----------
    model_parameters: pandas.DataFrame

    Returns
    --------
    None
    &#34;&#34;&#34;
    model_parameters = model_parameters.copy()
    model_parameters.rename(columns={&#34;experiment&#34;: &#34;workflow_id&#34;}, inplace=True)
    # can&#39;t store NaNs
    model_parameters = model_parameters.replace({np.nan: None})
    model_parameters[&#34;well&#34;] = utils.unpad_well_col(model_parameters[&#34;well&#34;])
    self.session.bulk_insert_mappings(
        db_models.NE_model_parameters, model_parameters.to_dict(orient=&#34;records&#34;)
    )</code></pre>
</details>
</dd>
<dt id="plaque_assay.data.DatabaseUploader.upload_normalised_results"><code class="name flex">
<span>def <span class="ident">upload_normalised_results</span></span>(<span>self, norm_results)</span>
</code></dt>
<dd>
<div class="desc"><p>Upload normalised results into the database.</p>
<p>Uploads the normalised data, consisting of
<code>background_subtracted_plaque_area</code>, <code>percentage_infected</code> and
metadata, into the LIMS serology database.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>norm_results</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def upload_normalised_results(self, norm_results):
    &#34;&#34;&#34;Upload normalised results into the database.

    Uploads the normalised data, consisting of 
    `background_subtracted_plaque_area`, `percentage_infected` and
    metadata, into the LIMS serology database.

    Parameters
    ----------
    norm_results: pandas.DataFrame

    Returns
    -------
    None
    &#34;&#34;&#34;
    norm_results = norm_results.copy()
    rename_dict = {
        &#34;Well&#34;: &#34;well&#34;,
        &#34;Row&#34;: &#34;row&#34;,
        &#34;Column&#34;: &#34;column&#34;,
        &#34;Dilution&#34;: &#34;dilution&#34;,
        &#34;Plate_barcode&#34;: &#34;plate_barcode&#34;,
        &#34;Background_subtracted_plaque_area&#34;: &#34;background_subtracted_plaque_area&#34;,
        &#34;Percentage_infected&#34;: &#34;percentage_infected&#34;,
        &#34;variant&#34;: &#34;variant&#34;,  # not renamed, just to keep
    }
    norm_results.rename(columns=rename_dict, inplace=True)
    norm_results = norm_results[list(rename_dict.values())]
    workflow_id = [int(i[3:]) for i in norm_results[&#34;plate_barcode&#34;]]
    assert len(set(workflow_id)) == 1
    norm_results[&#34;workflow_id&#34;] = workflow_id
    norm_results[&#34;well&#34;] = utils.unpad_well_col(norm_results[&#34;well&#34;])
    # can&#39;t store NaN in mysql, to convert to None which are stored as null
    norm_results = norm_results.replace({np.nan: None})
    self.session.bulk_insert_mappings(
        db_models.NE_normalized_results, norm_results.to_dict(orient=&#34;records&#34;)
    )</code></pre>
</details>
</dd>
<dt id="plaque_assay.data.DatabaseUploader.upload_plate_results"><code class="name flex">
<span>def <span class="ident">upload_plate_results</span></span>(<span>self, plate_results_dataset)</span>
</code></dt>
<dd>
<div class="desc"><p>Upload raw concatenated data into database.</p>
<p>This uploads the "raw" dataset into the LIMS serology database.
The data is largely what is exported from the Phenix with columns
renamed, some columns moved, some metadata columns added.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>plate_results_dataset</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns:</h2>
<p>None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def upload_plate_results(self, plate_results_dataset):
    &#34;&#34;&#34;Upload raw concatenated data into database.

    This uploads the &#34;raw&#34; dataset into the LIMS serology database.
    The data is largely what is exported from the Phenix with columns
    renamed, some columns moved, some metadata columns added.

    Parameters
    ----------
    plate_results_dataset: pandas.DataFrame

    Returns:
    ---------
    None
    &#34;&#34;&#34;
    # TODO: check csv matches master plate selected in NE_workflow_tracking
    plate_results_dataset = plate_results_dataset.copy()
    rename_dict = {
        &#34;Row&#34;: &#34;row&#34;,
        &#34;Column&#34;: &#34;column&#34;,
        &#34;Viral Plaques (global) - Area of Viral Plaques Area [µm²] - Mean per Well&#34;: &#34;VPG_area_mean&#34;,
        &#34;Viral Plaques (global) - Intensity Viral Plaques Alexa 488 (global) Mean - Mean per Well&#34;: &#34; VPG_intensity_mean_per_well&#34;,
        &#34;Viral Plaques (global) - Intensity Viral Plaques Alexa 488 (global) StdDev - Mean per Well&#34;: &#34;VPG_intensity_stddev_per_well&#34;,
        &#34;Viral Plaques (global) - Intensity Viral Plaques Alexa 488 (global) Median - Mean per Well&#34;: &#34;VPG_intensity_median_per_well&#34;,
        &#34;Viral Plaques (global) - Intensity Viral Plaques Alexa 488 (global) Sum - Mean per Well&#34;: &#34;VPG_intensity_sum_per_well&#34;,
        &#34;Cells - Intensity Image Region DAPI (global) Mean - Mean per Well&#34;: &#34;cells_intensity_mean_per_well&#34;,
        &#34;Cells - Intensity Image Region DAPI (global) StdDev - Mean per Well&#34;: &#34;cells_intensity_stddev_mean_per_well&#34;,
        &#34;Cells - Intensity Image Region DAPI (global) Median - Mean per Well&#34;: &#34;cells_intensity_median_mean_per_well&#34;,
        &#34;Cells - Intensity Image Region DAPI (global) Sum - Mean per Well&#34;: &#34;cells_intensity_sum_mean_per_well&#34;,
        &#34;Cells - Image Region Area [µm²] - Mean per Well&#34;: &#34;cells_image_region_area_mean_per_well&#34;,
        &#34;Normalised Plaque area&#34;: &#34;normalised_plaque_area&#34;,
        &#34;Normalised Plaque intensity&#34;: &#34;normalised_plaque_intensity&#34;,
        &#34;Number of Analyzed Fields&#34;: &#34;number_analyzed_fields&#34;,
        &#34;Dilution&#34;: &#34;dilution&#34;,
        &#34;Well&#34;: &#34;well&#34;,
        &#34;PlateNum&#34;: &#34;plate_num&#34;,
        &#34;Plate_barcode&#34;: &#34;plate_barcode&#34;,
        &#34;variant&#34;: &#34;variant&#34;,
        # &#34;Background Subtracted Plaque Area&#34;: &#34;background_subtracted_plaque_area&#34;,
    }
    plate_results_dataset.rename(columns=rename_dict, inplace=True)
    # filter to only desired columns
    plate_results_dataset = plate_results_dataset[list(rename_dict.values())]
    workflow_id = [int(i[3:]) for i in plate_results_dataset[&#34;plate_barcode&#34;]]
    plate_results_dataset[&#34;workflow_id&#34;] = workflow_id
    plate_results_dataset[&#34;well&#34;] = utils.unpad_well_col(
        plate_results_dataset[&#34;well&#34;]
    )
    # can&#39;t store NaN in mysql, to convert to None which are stored as null
    plate_results_dataset = plate_results_dataset.replace({np.nan: None})
    self.session.bulk_insert_mappings(
        db_models.NE_raw_results, plate_results_dataset.to_dict(orient=&#34;records&#34;)
    )</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="plaque_assay" href="index.html">plaque_assay</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="plaque_assay.data.get_plate_list" href="#plaque_assay.data.get_plate_list">get_plate_list</a></code></li>
<li><code><a title="plaque_assay.data.read_data_from_directory" href="#plaque_assay.data.read_data_from_directory">read_data_from_directory</a></code></li>
<li><code><a title="plaque_assay.data.read_data_from_list" href="#plaque_assay.data.read_data_from_list">read_data_from_list</a></code></li>
<li><code><a title="plaque_assay.data.read_indexfiles_from_directory" href="#plaque_assay.data.read_indexfiles_from_directory">read_indexfiles_from_directory</a></code></li>
<li><code><a title="plaque_assay.data.read_indexfiles_from_list" href="#plaque_assay.data.read_indexfiles_from_list">read_indexfiles_from_list</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="plaque_assay.data.DatabaseUploader" href="#plaque_assay.data.DatabaseUploader">DatabaseUploader</a></code></h4>
<ul class="">
<li><code><a title="plaque_assay.data.DatabaseUploader.already_uploaded" href="#plaque_assay.data.DatabaseUploader.already_uploaded">already_uploaded</a></code></li>
<li><code><a title="plaque_assay.data.DatabaseUploader.commit" href="#plaque_assay.data.DatabaseUploader.commit">commit</a></code></li>
<li><code><a title="plaque_assay.data.DatabaseUploader.is_final_upload" href="#plaque_assay.data.DatabaseUploader.is_final_upload">is_final_upload</a></code></li>
<li><code><a title="plaque_assay.data.DatabaseUploader.update_workflow_tracking" href="#plaque_assay.data.DatabaseUploader.update_workflow_tracking">update_workflow_tracking</a></code></li>
<li><code><a title="plaque_assay.data.DatabaseUploader.upload_failures" href="#plaque_assay.data.DatabaseUploader.upload_failures">upload_failures</a></code></li>
<li><code><a title="plaque_assay.data.DatabaseUploader.upload_final_results" href="#plaque_assay.data.DatabaseUploader.upload_final_results">upload_final_results</a></code></li>
<li><code><a title="plaque_assay.data.DatabaseUploader.upload_indexfiles" href="#plaque_assay.data.DatabaseUploader.upload_indexfiles">upload_indexfiles</a></code></li>
<li><code><a title="plaque_assay.data.DatabaseUploader.upload_model_parameters" href="#plaque_assay.data.DatabaseUploader.upload_model_parameters">upload_model_parameters</a></code></li>
<li><code><a title="plaque_assay.data.DatabaseUploader.upload_normalised_results" href="#plaque_assay.data.DatabaseUploader.upload_normalised_results">upload_normalised_results</a></code></li>
<li><code><a title="plaque_assay.data.DatabaseUploader.upload_plate_results" href="#plaque_assay.data.DatabaseUploader.upload_plate_results">upload_plate_results</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>